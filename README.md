Earnings Manipulation Detection using Machine Learning

An end-to-end Streamlit-based decision support system for detecting earnings manipulation risk in firms using classical accounting theory (Beneish M-Score) and modern machine learning models.

This project is designed as an early-warning screening tool, not a fraud confirmation system, and emphasizes recall-oriented detection, validation-based evaluation, and interpretability.

 Project Overview

Earnings manipulation poses significant risks to investors, auditors, and regulators. Traditional models such as the Beneish M-Score provide useful red flags but suffer from high false positives and limited adaptability.

This project enhances detection by:

Using machine learning classifiers trained on Beneish-style financial ratios

Explicitly separating training, validation, and test datasets

Performing validation-based threshold tuning

Comparing ML models against the Beneish benchmark

Providing firm-level risk assessment via an interactive web interface

Objectives

Detect firms with high likelihood of earnings manipulation

Minimize false negatives (missed manipulators)

Compare classical vs ML-based detection

Ensure methodological transparency and interpretability

Provide a decision-support dashboard, not a black-box predictor
Evaluation Strategy (Key Strength)
Dataset Splitting

The dataset is divided into three mutually exclusive subsets:

Training set â†’ Model fitting

Validation set â†’ Model selection & threshold tuning

Test set â†’ Final, unbiased performance reporting

This prevents information leakage and ensures reliable evaluation.

Primary Evaluation Metric

Recall (Sensitivity) is emphasized because:

Missing a manipulator (false negative) is more costly than false alarms

The system is intended for audit/regulatory screening

Supporting metrics:

F1 Score â€“ balances recall and precision

ROC-AUC â€“ model comparison

Accuracy â€“ reported but not emphasized due to class imbalance

Interpretability

SHAP (SHapley Additive exPlanations) is applied selectively:

âœ” Logistic Regression

âœ” Random Forest

âœ” XGBoost

SHAP is intentionally skipped for ensemble/stacking models due to lack of a unified attribution structure.

This ensures theoretical validity of explanations.

Beneish M-Score Benchmark

The Beneish M-Score is calculated using the original formula:

ð‘€
=
âˆ’
4.84
+
0.92
â‹…
ð·
ð‘†
ð‘…
ð¼
+
0.528
â‹…
ðº
ð‘€
ð¼
+
0.404
â‹…
ð´
ð‘„
ð¼
+
0.892
â‹…
ð‘†
ðº
ð¼
+
0.115
â‹…
ð·
ð¸
ð‘ƒ
ð¼
âˆ’
0.172
â‹…
ð‘†
ðº
ð´
ð¼
+
4.679
â‹…
ð´
ð¶
ð¶
ð‘…
âˆ’
0.327
â‹…
ð¿
ð¸
ð‘‰
ð¼
M=âˆ’4.84+0.92â‹…DSRI+0.528â‹…GMI+0.404â‹…AQI+0.892â‹…SGI+0.115â‹…DEPIâˆ’0.172â‹…SGAI+4.679â‹…ACCRâˆ’0.327â‹…LEVI

Firms with M-Score > âˆ’2.22 are flagged as likely manipulators

Used strictly as a reference benchmark, not the final decision rule
